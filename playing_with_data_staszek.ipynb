{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazing ML journey with Stan    \n",
    "Welcome! Let me show you around on this amazing experience.   \n",
    "FOR THE LOVE OF GOD DO NOT JUST RUN THE WHOLE THING!    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The path    \n",
    "First, make sure that you have created a .py file containing variable AUDIO_DATA_PATH with the path to the audio data, pointing to AUDIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paths\n",
    "# Create a .py file containing variable AUDIO_DATA_PATH with the path to the audio data, pointing to AUDIO\n",
    "AUDIO_DATA_PATH = paths.AUDIO_DATA_PATH\n",
    "# The structure of the data folder should be as follows:\n",
    "\n",
    "#         â†“   The path should be pointing to this folder\n",
    "# data/AUDIO/\n",
    "#           FAKE\n",
    "#           REAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wasil\\anaconda3\\envs\\ml01\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports go brrrrrr\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import wave\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from IPython import display\n",
    "from pydub import AudioSegment\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting audio    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into 10 second fragments and saving them to the AUDIO folder\n",
    "\n",
    "def split_audio(input_folder, output_folder, duration=10):\n",
    "\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mp3\") or file.endswith(\".wav\"):\n",
    "                input_path = os.path.join(root, file)\n",
    "                audio = AudioSegment.from_file(input_path)\n",
    "\n",
    "                for i, chunk in enumerate(audio[::duration * 1000]): # duration 10 seconds\n",
    "                    chunk_name = f\"{os.path.splitext(file)[0]}_{i}.wav\"\n",
    "                    output_path = os.path.join(output_folder, chunk_name)\n",
    "                    chunk.export(output_path, format=\"wav\")\n",
    "\n",
    "input_folder = AUDIO_DATA_PATH\n",
    "output_folder_real = os.path.join(AUDIO_DATA_PATH, \"NEW_REAL\")\n",
    "output_folder_fake = os.path.join(AUDIO_DATA_PATH, \"NEW_FAKE\")\n",
    "\n",
    "split_audio(os.path.join(input_folder, \"REAL\"), output_folder_real)\n",
    "split_audio(os.path.join(input_folder, \"FAKE\"), output_folder_fake)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving images  \n",
    "Now it's time to save all slices of audio as spectograms    \n",
    "(This might take a while, for me it was 12 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wasil\\AppData\\Local\\Temp\\ipykernel_6960\\3014043850.py:36: UserWarning: Only one segment is calculated since parameter NFFT (=256) >= signal length (=144).\n",
      "  plt.specgram(data, Fs=sample_rate)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "path = AUDIO_DATA_PATH\n",
    "\n",
    "fake_img = \"FAKE_IMG\"\n",
    "real_img = \"REAL_IMG\"\n",
    "\n",
    "# Create folders if they don't exist\n",
    "if not os.path.exists(fake_img):\n",
    "    os.makedirs(fake_img)\n",
    "if not os.path.exists(real_img):\n",
    "    os.makedirs(real_img)\n",
    "\n",
    "for subfolder in os.listdir(path):\n",
    "    if subfolder == \"NEW_FAKE\":\n",
    "        output = fake_img\n",
    "    elif subfolder == \"NEW_REAL\":\n",
    "        output = real_img\n",
    "    else:\n",
    "        continue\n",
    "    for file in os.listdir(os.path.join(path, subfolder)):\n",
    "        if file.endswith(\".wav\"):\n",
    "            sample_rate, data = wav.read(os.path.join(path, subfolder, file))\n",
    "            data = np.mean(data, axis=1) # Average the two channels for a mono channel\n",
    "            plt.specgram(data, Fs=sample_rate)\n",
    "            plt.savefig(os.path.join(output, file[:-4] + \".png\"))\n",
    "            plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train, test, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3013 files belonging to 2 classes.\n",
      "Using 2110 files for training.\n",
      "Found 3013 files belonging to 2 classes.\n",
      "Using 903 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_size = (128, 128)\n",
    "validation_split = 0.3\n",
    "seed_train_validation = 1\n",
    "shuffle_value = True\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='data/IMAGES',\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=seed_train_validation,\n",
    "    color_mode='grayscale', # for now, we will only use grayscale images\n",
    "    shuffle=shuffle_value\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='data/IMAGES',\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed_train_validation,\n",
    "    color_mode='grayscale',\n",
    "    shuffle=shuffle_value\n",
    ")\n",
    "\n",
    "# number of batches in the validation set\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "\n",
    "# split validation into test and validation sets\n",
    "test_ds = val_ds.take((2 * val_batches) // 3)\n",
    "val_ds = val_ds.skip((2 * val_batches) // 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 63, 63, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 30, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 14, 14, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               3211392   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3304193 (12.60 MB)\n",
      "Trainable params: 3304193 (12.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten the output so it can go from CNN to Dense\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # DENSEEEE\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),  # We don't really need that but it might help, idk\n",
    "    layers.Dense(1, activation='sigmoid')  # Sigmoid so we can set a threshold\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', # We can try different things here\n",
    "              loss='binary_crossentropy', # Here also we can try something different\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66/66 [==============================] - 18s 253ms/step - loss: 8.8397 - accuracy: 0.8223 - val_loss: 0.4837 - val_accuracy: 0.8339\n",
      "Epoch 2/10\n",
      "66/66 [==============================] - 18s 262ms/step - loss: 0.4257 - accuracy: 0.8730 - val_loss: 0.4257 - val_accuracy: 0.8441\n",
      "Epoch 3/10\n",
      "66/66 [==============================] - 18s 269ms/step - loss: 0.4059 - accuracy: 0.8730 - val_loss: 0.3732 - val_accuracy: 0.8542\n",
      "Epoch 4/10\n",
      "66/66 [==============================] - 18s 276ms/step - loss: 0.3590 - accuracy: 0.8725 - val_loss: 0.3437 - val_accuracy: 0.8576\n",
      "Epoch 5/10\n",
      "66/66 [==============================] - 18s 273ms/step - loss: 0.3431 - accuracy: 0.8697 - val_loss: 0.2842 - val_accuracy: 0.8780\n",
      "Epoch 6/10\n",
      "66/66 [==============================] - 18s 272ms/step - loss: 0.3375 - accuracy: 0.8725 - val_loss: 0.3920 - val_accuracy: 0.9627\n",
      "Epoch 7/10\n",
      "66/66 [==============================] - 18s 274ms/step - loss: 0.3167 - accuracy: 0.9118 - val_loss: 0.2582 - val_accuracy: 0.9356\n",
      "Epoch 8/10\n",
      "66/66 [==============================] - 18s 276ms/step - loss: 0.2859 - accuracy: 0.9185 - val_loss: 0.1752 - val_accuracy: 0.9492\n",
      "Epoch 9/10\n",
      "66/66 [==============================] - 18s 271ms/step - loss: 0.2757 - accuracy: 0.9142 - val_loss: 0.2134 - val_accuracy: 0.9356\n",
      "Epoch 10/10\n",
      "66/66 [==============================] - 18s 273ms/step - loss: 0.2773 - accuracy: 0.9180 - val_loss: 0.2028 - val_accuracy: 0.9356\n",
      "19/19 [==============================] - 2s 77ms/step - loss: 0.1941 - accuracy: 0.9441\n",
      "Test accuracy: 0.9441\n"
     ]
    }
   ],
   "source": [
    "# (train_ds, val_ds, test_ds)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Train the shit out of it\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_sizes, train_scores, test_scores \u001b[38;5;241m=\u001b[39m learning_curve(\n\u001b[1;32m----> 2\u001b[0m     model, \u001b[43mX\u001b[49m, y, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_sizes, train_scores\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Score\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "The image is predicted as FAKE.\n"
     ]
    }
   ],
   "source": [
    "# Fake\n",
    "\n",
    "test_image_path = 'data/IMAGES/FAKE_IMG/biden-to-linus_2.png'\n",
    "test_image = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE) # again, grayscale\n",
    "test_image = cv2.resize(test_image, (128, 128))\n",
    "test_image = np.expand_dims(test_image, axis=0) # batch dimension\n",
    "\n",
    "\n",
    "prediction = model.predict(test_image)\n",
    "\n",
    "if prediction[0][0] >= 0.5: # the sensitivity can be changed\n",
    "    print(\"The image is predicted as REAL.\")\n",
    "else:\n",
    "    print(\"The image is predicted as FAKE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "The image is predicted as REAL.\n"
     ]
    }
   ],
   "source": [
    "# Real\n",
    "\n",
    "test_image_path = 'data/IMAGES/REAL_IMG/biden-original_0.png'\n",
    "test_image = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE) # again, grayscale\n",
    "test_image = cv2.resize(test_image, (128, 128))\n",
    "test_image = np.expand_dims(test_image, axis=0) # batch dimension\n",
    "\n",
    "\n",
    "prediction = model.predict(test_image)\n",
    "\n",
    "if prediction[0][0] >= 0.5: # the sensitivity can be changed\n",
    "    print(\"The image is predicted as REAL.\")\n",
    "else:\n",
    "    print(\"The image is predicted as FAKE.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
