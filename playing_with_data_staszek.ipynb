{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazing ML journey with Stan    \n",
    "Welcome! Let me show you around on this amazing experience.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The path    \n",
    "First, make sure that you have created a .py file containing variable AUDIO_DATA_PATH with the path to the audio data, pointing to AUDIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paths\n",
    "# Create a .py file containing variable AUDIO_DATA_PATH with the path to the audio data, pointing to AUDIO\n",
    "AUDIO_DATA_PATH = paths.AUDIO_DATA_PATH\n",
    "# The structure of the data folder should be as follows:\n",
    "\n",
    "#         â†“   The path should be pointing to this folder\n",
    "# data/AUDIO/\n",
    "#           FAKE\n",
    "#           REAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wasil\\anaconda3\\envs\\ml01\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Imports go brrrrrr\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import wave\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from IPython import display\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting audio    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into 10 second fragments and saving them to the AUDIO folder\n",
    "\n",
    "def split_audio(input_folder, output_folder, duration=10):\n",
    "\n",
    "    for root, _, files in os.walk(input_folder):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mp3\") or file.endswith(\".wav\"):\n",
    "                input_path = os.path.join(root, file)\n",
    "                audio = AudioSegment.from_file(input_path)\n",
    "\n",
    "                for i, chunk in enumerate(audio[::duration * 1000]): # duration 10 seconds\n",
    "                    chunk_name = f\"{os.path.splitext(file)[0]}_{i}.wav\"\n",
    "                    output_path = os.path.join(output_folder, chunk_name)\n",
    "                    chunk.export(output_path, format=\"wav\")\n",
    "\n",
    "input_folder = AUDIO_DATA_PATH\n",
    "output_folder_real = os.path.join(AUDIO_DATA_PATH, \"NEW_REAL\")\n",
    "output_folder_fake = os.path.join(AUDIO_DATA_PATH, \"NEW_FAKE\")\n",
    "\n",
    "split_audio(os.path.join(input_folder, \"REAL\"), output_folder_real)\n",
    "split_audio(os.path.join(input_folder, \"FAKE\"), output_folder_fake)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving images  \n",
    "Now it's time to save all slices of audio as spectograms    \n",
    "(This might take a while, for me it was 12 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wasil\\AppData\\Local\\Temp\\ipykernel_6960\\3014043850.py:36: UserWarning: Only one segment is calculated since parameter NFFT (=256) >= signal length (=144).\n",
      "  plt.specgram(data, Fs=sample_rate)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "path = AUDIO_DATA_PATH\n",
    "\n",
    "fake_img = \"FAKE_IMG\"\n",
    "real_img = \"REAL_IMG\"\n",
    "\n",
    "# Create folders if they don't exist\n",
    "if not os.path.exists(fake_img):\n",
    "    os.makedirs(fake_img)\n",
    "if not os.path.exists(real_img):\n",
    "    os.makedirs(real_img)\n",
    "\n",
    "for subfolder in os.listdir(path):\n",
    "    if subfolder == \"NEW_FAKE\":\n",
    "        output = fake_img\n",
    "    elif subfolder == \"NEW_REAL\":\n",
    "        output = real_img\n",
    "    else:\n",
    "        continue\n",
    "    for file in os.listdir(os.path.join(path, subfolder)):\n",
    "        if file.endswith(\".wav\"):\n",
    "            sample_rate, data = wav.read(os.path.join(path, subfolder, file))\n",
    "            data = np.mean(data, axis=1) # Average the two channels for a mono channel\n",
    "            plt.specgram(data, Fs=sample_rate)\n",
    "            plt.savefig(os.path.join(output, file[:-4] + \".png\"))\n",
    "            plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into train, test, and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3013 files belonging to 2 classes.\n",
      "Using 2110 files for training.\n",
      "Found 3013 files belonging to 2 classes.\n",
      "Using 903 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "image_size = (128, 128)\n",
    "validation_split = 0.3\n",
    "seed_train_validation = 1\n",
    "shuffle_value = True\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='data/IMAGES',\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=seed_train_validation,\n",
    "    color_mode='grayscale', # for now, we will only use grayscale images\n",
    "    shuffle=shuffle_value\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    directory='data/IMAGES',\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed_train_validation,\n",
    "    color_mode='grayscale',\n",
    "    shuffle=shuffle_value\n",
    ")\n",
    "\n",
    "# number of batches in the validation set\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "\n",
    "# split validation into test and validation sets\n",
    "test_ds = val_ds.take((2 * val_batches) // 3)\n",
    "val_ds = val_ds.skip((2 * val_batches) // 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 63, 63, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 30, 30, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 14, 14, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               3211392   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3304193 (12.60 MB)\n",
      "Trainable params: 3304193 (12.60 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    # Convolutional layers\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten the output for Dense layers\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # DENSEEEE\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),  # Optional: Add dropout for regularization\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification output\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', # we can try different things\n",
    "              loss='binary_crossentropy', # Here also we can try something different\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "66/66 [==============================] - 22s 314ms/step - loss: 6.0870 - accuracy: 0.8417 - val_loss: 0.3802 - val_accuracy: 0.8339\n",
      "Epoch 2/4\n",
      "66/66 [==============================] - 19s 285ms/step - loss: 0.3320 - accuracy: 0.8915 - val_loss: 0.3027 - val_accuracy: 0.9254\n",
      "Epoch 3/4\n",
      "66/66 [==============================] - 21s 307ms/step - loss: 0.2954 - accuracy: 0.9280 - val_loss: 0.2048 - val_accuracy: 0.9424\n",
      "Epoch 4/4\n",
      "66/66 [==============================] - 20s 294ms/step - loss: 0.2450 - accuracy: 0.9370 - val_loss: 0.2214 - val_accuracy: 0.9390\n",
      "19/19 [==============================] - 2s 85ms/step - loss: 0.2420 - accuracy: 0.9424\n",
      "Test accuracy: 0.9424\n"
     ]
    }
   ],
   "source": [
    "# (train_ds, val_ds, test_ds)\n",
    "\n",
    "epochs = 4\n",
    "batch_size = 32\n",
    "\n",
    "# Train the shit out of it\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step\n",
      "The image is predicted as FAKE.\n"
     ]
    }
   ],
   "source": [
    "# Fake\n",
    "\n",
    "test_image_path = 'data/IMAGES/FAKE_IMG/biden-to-linus_2.png'\n",
    "test_image = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE) # again, grayscale\n",
    "test_image = cv2.resize(test_image, (128, 128))\n",
    "test_image = np.expand_dims(test_image, axis=0) # batch dimension\n",
    "\n",
    "\n",
    "prediction = model.predict(test_image)\n",
    "\n",
    "if prediction[0][0] >= 0.5: # the sensitivity can be changed\n",
    "    print(\"The image is predicted as REAL.\")\n",
    "else:\n",
    "    print(\"The image is predicted as FAKE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "The image is predicted as REAL.\n"
     ]
    }
   ],
   "source": [
    "# Real\n",
    "\n",
    "test_image_path = 'data/IMAGES/REAL_IMG/biden-original_0.png'\n",
    "test_image = cv2.imread(test_image_path, cv2.IMREAD_GRAYSCALE) # again, grayscale\n",
    "test_image = cv2.resize(test_image, (128, 128))\n",
    "test_image = np.expand_dims(test_image, axis=0) # batch dimension\n",
    "\n",
    "\n",
    "prediction = model.predict(test_image)\n",
    "\n",
    "if prediction[0][0] >= 0.5: # the sensitivity can be changed\n",
    "    print(\"The image is predicted as REAL.\")\n",
    "else:\n",
    "    print(\"The image is predicted as FAKE.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
